{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Networks in Parallel via a Single Solver\n",
    "\n",
    "Imagine the following scenarios:\n",
    "\n",
    "You're experimenting with a network on a given datasert, **same data, different networks**.\n",
    "\n",
    "You're still trying to figure out the right topology (e.g. number of filters, number of conv. layers, etc.)\n",
    "\n",
    "Ways to go about this:\n",
    "\n",
    "### 1. Sequential training:\n",
    "\n",
    "You configure your network. Trian it. Decide on what to do next. Re-configure accordingly, then train again.\n",
    "\n",
    "**Pros**: You make decisions based on previous results. Your experimentation is somehwat **guided**. For small networks, you're not really utilizing the full potential of your resources (e.g. GPU RAM, idle GPU cores.)\n",
    "\n",
    "**Cons**: This is **purely sequential** and narrowing things down can take a very long time.\n",
    "\n",
    "### 2. Run training processes in parallel:\n",
    "\n",
    "You define a different network definitions (e.g. trainval_XX.prototxt). You configure each differently.\n",
    "You start a training process for each. The processes run in parallel on the same GPU.\n",
    "\n",
    "**Pros**: You don't have to wait till one experiment is finished to launch the next. No resources left idle.\n",
    "\n",
    "**Cons**: All processes are competing for the same resource. There might be a sweet spot between parallel processing and avoding overhead due to context switching. The bigger problem is **redundancy**, especially if all networks are working off of the **same data**. Each of these processes has to do its own data copy from CPU to GPU RAM. The redundancy only affects training on GPU. CPU optimization doesn't have this issue, it's already slow enough as it is.\n",
    "\n",
    "Below we demonstrate training networks in *Caffe* off of the same data in parrallel without the redundant data copy cost. Our networks all work off of the same data but have different architectures/configurations. Or they're all replicas and we're just looking at the effect of different random weight initializations.\n",
    "The point is to pay the cost of copying the data from CPU to GPU once for all networks.\n",
    "\n",
    "We'll do this by defining a single solver for all. And merging the different network definitions into one.\n",
    "All layers that are not data layers (e.g. conv layer, ip layers, pooling, loss layers) need to have ***unique layer names + unique top names***.\n",
    "\n",
    "This example also verifies that training two networks in parallel leads to same results as training them in sequency. Networks that share a prototxt do not influence one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from proto.proto_utils import Parser\n",
    "from nets.net_merge import merge_indep_net_spec\n",
    "import caffe\n",
    "from caffe import layers as L, params as P\n",
    "    \n",
    "print(\"Done importing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define two toy networks programmatically\n",
    "\n",
    "Partially from the caffe notebook example [Learning LeNet](http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb)\n",
    "The network definition is simplified to speed things up\n",
    "We define two networks for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating network prototxts.\n"
     ]
    }
   ],
   "source": [
    "def lenet_simple(lmdb_train, lmdb_test, batch_size):\n",
    "    # partially from the caffe notebook example \"Learning LeNet\"\n",
    "    # source: http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb\n",
    "    # network simplified to speed things up\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    if not os.path.isdir(lmdb_train):\n",
    "        raise(IOError, \"source for training data (%s) does not exist!\" % (lmdb_train,))\n",
    "    if not os.path.isdir(lmdb_test):\n",
    "        raise(IOError, \"source for test data (%s) does not exist!\" % (lmdb_test,))\n",
    "    \n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = \\\n",
    "    L.Data(batch_size=100, backend=P.Data.LMDB,\n",
    "           source=lmdb_train,\n",
    "           transform_param=dict(scale=0.0039215684),\n",
    "           ntop=2,\n",
    "           include=[dict(phase=caffe.TRAIN)]\n",
    "           )\n",
    "    # will fix naming of data layer blobs for test phase after defining remaining layers\n",
    "    n.data_test, n.label_test = \\\n",
    "        L.Data(batch_size=100, backend=P.Data.LMDB,\n",
    "               source=lmdb_test,\n",
    "               transform_param=dict(scale=0.0039215684),\n",
    "               ntop=2,\n",
    "               include=[dict(phase=caffe.TEST)]\n",
    "               )\n",
    "    \n",
    "    n.ip = L.InnerProduct(n.data, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.SoftmaxWithLoss(n.ip, n.label)\n",
    "    \n",
    "    n_proto = n.to_proto()\n",
    "    \n",
    "    # fix data layer for test phase\n",
    "    for l in n_proto.layer:\n",
    "        if l.type.lower() == 'data' and \\\n",
    "           [x.phase for x in l.include] == [caffe.TEST]:\n",
    "            for t in list(l.top):\n",
    "                l.top.remove(t)\n",
    "                t = t.split('_test')[0]\n",
    "                l.top.append(unicode(t))\n",
    "            l.name = l.name.split('_test')[0]\n",
    "    \n",
    "    return n_proto\n",
    "\n",
    "dir_dst = '../parallel_train/tmp/'\n",
    "if not os.path.isdir(dir_dst):\n",
    "    os.makedirs(dir_dst) # create subdirectory\n",
    "# generate definitions of multiple networks\n",
    "fpath_net0 = os.path.join(dir_dst, 'trainval0.prototxt') # will be generated\n",
    "with open(fpath_net0, 'w') as f:\n",
    "    f.write(str(lenet_simple(os.path.expanduser('~/data/mnist/mnist_train_lmdb'),\n",
    "                             os.path.expanduser('~/data/mnist/mnist_test_lmdb'),\n",
    "                             64)))\n",
    "\n",
    "# define a 2nd network (coincidentally identical to first)\n",
    "fpath_net1 = os.path.join(dir_dst, 'trainval1.prototxt') # will be generated\n",
    "with open(fpath_net1, 'w') as f:\n",
    "    f.write(str(lenet_simple(os.path.expanduser('~/data/mnist/mnist_train_lmdb'),\n",
    "                             os.path.expanduser('~/data/mnist/mnist_test_lmdb'),\n",
    "                             64)))\n",
    "\n",
    "print(\"Done generating network prototxts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to show that training in parallel leads to the same results as training in sequence.\n",
    "To do so we'll:\n",
    "\n",
    "1. define a function to save weights and biases in a network to file\n",
    "2. define a function to save the **initial weights and biases** of a network to file.\n",
    "3. define a function to save the weights and biases **after a few learning steps**.\n",
    "4. !!! define a function to **merge two arbitrary network definitions** into a single prototxt !!!\n",
    "5. define a function to overwrite the inital weights of a network, train for a while then save the weights and biases\n",
    "\n",
    "Note: This is not the ideal way of save/load weights in *Caffe*. *Caffe* offers a much better API for doing these kind of things. We define these functions for demonstration and quick verification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def save_param(net, key, dir_dst, suffix):\n",
    "    \"\"\"\n",
    "    Save weights and biases in a network to file\n",
    "    \"\"\"\n",
    "    w = np.copy(net.params[key][0].data)\n",
    "    b = np.copy(net.params[key][1].data)\n",
    "    fpath_dst = os.path.join(dir_dst, '%s_%s.npz' % (key, suffix))\n",
    "    np.savez(fpath_dst, w=w, b=b)\n",
    "    return fpath_dst\n",
    "    \n",
    "def save_init_param(fpath_solver, key, dir_dst, suffix):\n",
    "    \"\"\"\n",
    "    Save the initial weights and biases of a network to file.\n",
    "    \"\"\"\n",
    "    solver = caffe.SGDSolver(fpath_solver)\n",
    "    return save_param(solver.net, key, dir_dst, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights and biases of\n",
      "network \"0\" saved to: ../parallel_train/tmp/ip/ip_0.npz\n",
      "network \"1\" saved to: ../parallel_train/tmp/ip/ip_1.npz\n"
     ]
    }
   ],
   "source": [
    "fpath_s0 = \"../parallel_train/solver0.prototxt\" # must exist\n",
    "fpath_s1 = \"../parallel_train/solver1.prototxt\" # must exist\n",
    "\n",
    "dir_dst = '../parallel_train/tmp/ip'\n",
    "if not os.path.isdir(dir_dst):\n",
    "    os.makedirs(dir_dst) # create subdirectory\n",
    "fpath_ip0 = save_init_param(fpath_s0, 'ip', dir_dst, '0')\n",
    "fpath_ip1 = save_init_param(fpath_s1, 'ip', dir_dst, '1')\n",
    "\n",
    "print(\"Initial weights and biases of\")\n",
    "print('network \\\"0\\\" saved to: %s' % (fpath_ip0,))\n",
    "print('network \\\"1\\\" saved to: %s' % (fpath_ip1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform n training steps and save intermediate values of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip\n",
      "[[ 0.05788097  0.02536247  0.03783927 ...,  0.05660211  0.03306225\n",
      "  -0.04104435]\n",
      " [-0.00567849  0.00575791 -0.03933674 ..., -0.05032981  0.01894811\n",
      "   0.01277347]\n",
      " [-0.01231461 -0.0501898  -0.01190227 ..., -0.01161266 -0.00670796\n",
      "  -0.02583147]\n",
      " ..., \n",
      " [ 0.00824084  0.05121026 -0.04818993 ...,  0.0186904   0.0527554\n",
      "   0.04773097]\n",
      " [-0.02646933 -0.04551386 -0.05093502 ..., -0.04702529  0.05106469\n",
      "  -0.0054607 ]\n",
      " [-0.03486231 -0.05131749 -0.02674159 ..., -0.02417965 -0.03152768\n",
      "  -0.05476236]]\n",
      "[-0.00150471 -0.00129049  0.00271865 -0.00013088  0.00024002 -0.00424293\n",
      " -0.00042385  0.00182542 -0.00097998  0.00378875]\n",
      "ip\n",
      "[[-0.0470377  -0.00183553 -0.03747605 ...,  0.0592008   0.00641022\n",
      "  -0.03487132]\n",
      " [ 0.01765465 -0.03570665  0.01032893 ...,  0.02926776 -0.05124031\n",
      "  -0.0522294 ]\n",
      " [ 0.03997116 -0.03270002 -0.06129517 ...,  0.06030603 -0.0568833\n",
      "   0.01372554]\n",
      " ..., \n",
      " [ 0.00655881 -0.01598678 -0.04183325 ...,  0.02304253  0.03658488\n",
      "  -0.02717597]\n",
      " [-0.05533621 -0.04034351  0.03046663 ...,  0.02740273 -0.0246003\n",
      "   0.03383166]\n",
      " [-0.02728209 -0.01449388 -0.01941105 ...,  0.00589001  0.03685413\n",
      "   0.01307978]]\n",
      "[ 0.00224969  0.00303182 -0.00155976 -0.00104922 -0.00371704  0.00202894\n",
      " -0.00078172  0.00137098 -0.00360355  0.00202984]\n",
      "Intermediate weights and biases of\n",
      "network \"0\" saved to: ../parallel_train/tmp/ip/ip_0_i.npz\n",
      "network \"1\" saved to: ../parallel_train/tmp/ip/ip_0_i.npz\n"
     ]
    }
   ],
   "source": [
    "def init_step_save_param(fpath_solver, fpath_ip, n, key, dir_dst, suffix):\n",
    "    \"\"\"\n",
    "    Save the weights and biases after a few learning steps.\n",
    "    \"\"\"\n",
    "    solver = caffe.SGDSolver(fpath_solver)\n",
    "    \n",
    "    # overwrite weights and biases\n",
    "    ip = np.load(fpath_ip)\n",
    "    solver.net.params[key][0].data[...] = np.copy(ip['w'])\n",
    "    solver.net.params[key][1].data[...] = np.copy(ip['b'])\n",
    "    \n",
    "    solver.step(n)\n",
    "    \n",
    "    #print key\n",
    "    #print(solver.net.params[key][0].data)\n",
    "    #print(solver.net.params[key][1].data)\n",
    "    \n",
    "    return save_param(solver.net, key, dir_dst, suffix)\n",
    "\n",
    "n = 5 # no. of training steps\n",
    "fpath_ip0_i = init_step_save_param(fpath_s0, fpath_ip0, n, 'ip', dir_dst, '0_i')\n",
    "fpath_ip1_i = init_step_save_param(fpath_s1, fpath_ip1, n, 'ip', dir_dst, '1_i')\n",
    "\n",
    "print(\"Intermediate weights and biases of\")\n",
    "print('network \\\"0\\\" saved to: %s' % (fpath_ip0_i,))\n",
    "print('network \\\"1\\\" saved to: %s' % (fpath_ip0_i,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge both networks into a single prototxt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged network with independent tracks defined in ../parallel_train/tmp/m.prototxt\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "def merge_nets(fpath_net_1, fpath_net_2, fpath_dst):\n",
    "    \"\"\"\n",
    "    Merge two arbitrary network definitions into a single prototxt.\n",
    "    \"\"\"\n",
    "    n1 = Parser().from_net_params_file(fpath_net_1)\n",
    "    n2 = Parser().from_net_params_file(fpath_net_2)\n",
    "    n_str = merge_indep_net_spec([n1, n2])\n",
    "    \n",
    "    with open(fpath_dst, 'w') as f:\n",
    "        f.write(n_str)\n",
    "    return\n",
    "##################################################################\n",
    "\n",
    "fpath_m = \"../parallel_train/tmp/m.prototxt\" # will be created\n",
    "merge_nets(fpath_net0, fpath_net1, fpath_m)\n",
    "print(\"Merged network with independent tracks defined in %s\" % (fpath_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train merged network for the same number of iterations.\n",
    "Then save intermediate weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05788097  0.02536247  0.03783927 ...,  0.05660211  0.03306225\n",
      "  -0.04104435]\n",
      " [-0.00567849  0.00575791 -0.03933674 ..., -0.05032981  0.01894811\n",
      "   0.01277347]\n",
      " [-0.01231461 -0.0501898  -0.01190227 ..., -0.01161266 -0.00670796\n",
      "  -0.02583147]\n",
      " ..., \n",
      " [ 0.00824084  0.05121026 -0.04818993 ...,  0.0186904   0.0527554\n",
      "   0.04773097]\n",
      " [-0.02646933 -0.04551386 -0.05093502 ..., -0.04702529  0.05106469\n",
      "  -0.0054607 ]\n",
      " [-0.03486231 -0.05131749 -0.02674159 ..., -0.02417965 -0.03152768\n",
      "  -0.05476236]]\n",
      "[-0.00150471 -0.00129049  0.00271865 -0.00013088  0.00024002 -0.00424293\n",
      " -0.00042385  0.00182542 -0.00097998  0.00378875]\n",
      "Intermediate weights and biases of merged network saved to\n",
      "../parallel_train/tmp/ip/ip_nidx_00_m_i.npz and ../parallel_train/tmp/ip/ip_nidx_01_m_i.npz\n"
     ]
    }
   ],
   "source": [
    "def init_merged_net_and_step_and_save(fpath_m, fpath_ip0, fpath_ip1, n, key, dir_dst, suffix):\n",
    "    \"\"\"\n",
    "    Overwrite the inital weights of a network, train for a while then save the weights and biases\n",
    "    \"\"\"\n",
    "    solver = caffe.SGDSolver(fpath_m)\n",
    "    \n",
    "    # overwrite weights and biases\n",
    "    ip0 = np.load(fpath_ip0)\n",
    "    solver.net.params[key+'_nidx_00'][0].data[...] = np.copy(ip0['w'])\n",
    "    solver.net.params[key+'_nidx_00'][1].data[...] = np.copy(ip0['b'])\n",
    "    \n",
    "    assert(np.all(solver.net.params[key+'_nidx_00'][0].data == ip0['w']))\n",
    "    assert(np.all(solver.net.params[key+'_nidx_00'][1].data == ip0['b']))\n",
    "    \n",
    "    ip1 = np.load(fpath_ip1)\n",
    "    solver.net.params[key+'_nidx_01'][0].data[...] = np.copy(ip1['w'])\n",
    "    solver.net.params[key+'_nidx_01'][1].data[...] = np.copy(ip1['b'])\n",
    "    \n",
    "    assert(np.all(solver.net.params[key+'_nidx_01'][0].data == ip1['w']))\n",
    "    assert(np.all(solver.net.params[key+'_nidx_01'][1].data == ip1['b']))\n",
    "    \n",
    "    # do some training\n",
    "    solver.step(n)\n",
    "    # check training lead to changed weights\n",
    "    assert(not np.all(solver.net.params[key+'_nidx_01'][0].data == ip0['w']))\n",
    "    assert(not np.all(solver.net.params[key+'_nidx_01'][1].data == ip0['b']))\n",
    "    \n",
    "    assert(not np.all(solver.net.params[key+'_nidx_01'][0].data == ip1['w']))\n",
    "    assert(not np.all(solver.net.params[key+'_nidx_01'][1].data == ip1['b']))\n",
    "    \n",
    "    #print(solver.net.params[key+'_nidx_00'][0].data)\n",
    "    #print(solver.net.params[key+'_nidx_00'][1].data)\n",
    "    \n",
    "    return [save_param(solver.net, key+'_nidx_00', dir_dst, suffix),\n",
    "            save_param(solver.net, key+'_nidx_01', dir_dst, suffix)]\n",
    "\n",
    "\n",
    "fpath_solver_m = \"../parallel_train/solver_m.prototxt\" # must exist\n",
    "    \n",
    "fpath_ip0m_i, fpath_ip1m_i = \\\n",
    "init_merged_net_and_step_and_save(fpath_solver_m, fpath_ip0, fpath_ip1, n, 'ip', dir_dst, 'm_i')\n",
    "\n",
    "print(\"Intermediate weights and biases of merged network saved to\")\n",
    "print('%s and %s' % (fpath_ip0m_i, fpath_ip1m_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that merged network arrived at the exact same weight and bias values as the sequential training of the sub networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are identical for sub network 0\n",
      "Biases are identical for sub network 0\n",
      "Weights are identical for sub network 1\n",
      "Biases are identical for sub network 1\n",
      "\n",
      "Training two networks in parallel by merging them into the same prototxt\n",
      "    results in same results as training them sequentially\n"
     ]
    }
   ],
   "source": [
    "ip0m_i = np.load(fpath_ip0m_i)\n",
    "ip0_i = np.load(fpath_ip0_i)\n",
    "\n",
    "if np.all(ip0m_i['w']==ip0_i['w']):\n",
    "    print(\"Weights are identical for sub network 0\")\n",
    "else:\n",
    "    print('ip0m_i weights', ip0m_i['w'])\n",
    "    print('ip0_i weights', ip0_i['w'])\n",
    "    raise(ValueError, \"Weights mismatch for sub network 0!\")\n",
    "    \n",
    "if np.all(ip0m_i['b']==ip0_i['b']):\n",
    "    print(\"Biases are identical for sub network 0\")\n",
    "else:\n",
    "    raise(ValueError, \"Bias mismatch for sub network 0!\")\n",
    "\n",
    "ip1m_i = np.load(fpath_ip1m_i)\n",
    "ip1_i = np.load(fpath_ip1_i)\n",
    "\n",
    "if np.all(ip1m_i['w']==ip1_i['w']):\n",
    "    print(\"Weights are identical for sub network 1\")\n",
    "else:\n",
    "    raise(ValueError, \"Weights mismatch for sub network 1!\")\n",
    "    \n",
    "if np.all(ip1m_i['b']==ip1_i['b']):\n",
    "    print(\"Biases are identical for sub network 1\")\n",
    "else:\n",
    "    raise(ValueError, \"Bias mismatch for sub network 1!\")\n",
    "\n",
    "# sanity cross-check\n",
    "assert(not np.all(ip0m_i['w']==ip1_i['w']))\n",
    "assert(not np.all(ip1m_i['w']==ip0_i['w']))\n",
    "assert(not np.all(ip0m_i['b']==ip1_i['b']))\n",
    "assert(not np.all(ip1m_i['b']==ip0_i['b']))\n",
    "\n",
    "# if we reach here, all's gone well\n",
    "print(\"\"\"\\nTraining two networks in parallel by merging them into the same prototxt\n",
    "    results in same results as training them sequentially\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
